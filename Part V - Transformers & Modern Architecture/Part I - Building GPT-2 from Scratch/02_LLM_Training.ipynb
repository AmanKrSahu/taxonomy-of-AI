{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeb068d-92a9-4092-85bc-6c0a872648b9",
   "metadata": {},
   "source": [
    "# LLM Training from Scratch (GPT-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655e7395-bf1c-48da-9683-c3a33449e9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from gpt2_model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6ff3344-586d-4fe2-83ce-10436dd86859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2754a96c-57d5-45fa-89f8-46792ded951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6acfc6ff-8079-4932-a075-c7ae4f093299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58954bc7-5e65-4410-b926-86cbf429cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22303ff-9fc2-470c-977c-f9aa6d1159aa",
   "metadata": {},
   "source": [
    "## 1. Generating Text from Output Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b7381-ba86-444b-80c1-6121032c4b45",
   "metadata": {},
   "source": [
    "Let us implement the token-generation process as follows:\n",
    "\n",
    "<b>Step 1</b>: idx is a (batch, n_tokens) array of indices in the current context\n",
    "\n",
    "<b>Step 2</b>: Crop current context if it exceeds the supported context size E.g., if LLM supports only 5 tokens, and the context size is 10 then only the last 5 tokens are used as context\n",
    "\n",
    "<b>Step 3</b>: Focus only on the last time step, so that (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "\n",
    "<b>Step 4</b>: probas has shape (batch, vocab_size)\n",
    "\n",
    "<b>Step 5</b>: idx_next has shape (batch, 1)\n",
    "\n",
    "<b>Step 6</b>: Append sampled index to the running sequence, where idx has shape (batch, n_tokens+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a75273-6d45-49a8-b0bf-4e7c46496b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (batch, n_tokens) array of indices in the current context\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest probability value\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260ed80-7655-4086-8895-f8f36e675c47",
   "metadata": {},
   "source": [
    "In the preceeding code, the generate_text_simple function, we use a softmax function to convert the logits into a probability distribution from which we identify the position with the highest value via torch.argmax. \n",
    "\n",
    "The softmax function is monotonic, meaning it preserves the order of its inputs when transformed into outputs. So, in practice, the softmax step is redundant since the position with the highest score in the softmax output tensor is the same position in the logit tensor.\n",
    "\n",
    "In other words, we could apply the torch.argmax function to the logits tensor directly and get identical results. However, we coded the conversion to illustrate the full process of transforming logits to probabilities, which can add additional intuition, such as that the model generates the most likely next token, which is known as greedy decoding.\n",
    "\n",
    "In the next chapter, when we will implement the GPT training code, we will also introduce additional sampling techniques where we modify the softmax outputs such that the model doesn't always select the most likely token, which introduces variability and creativity in the generated text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6b543-97df-4ecb-b5a4-f153e0f68cf8",
   "metadata": {},
   "source": [
    "Let's now try out the generate_text_simple function with the \"Hello, I am\" context as model input.\n",
    "\n",
    "First, we encode the input context into token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4efdf2b0-ccff-4216-8e58-ac52f6bee239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad5df6de-54e0-42f6-a51b-09a9ae4b7fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) #A\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e9f91f-7504-4e7b-b634-2717ae733762",
   "metadata": {},
   "source": [
    "Next, we put the model into .eval() mode, which disables random components like dropout, which are only used during training, and use the generate_text_simple function on the encoded input tensor.\n",
    "\n",
    "We disable dropout since we are not training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd96617-1834-41f0-9610-8b25bed816d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 10308, 49402, 13883, 19250,  6304, 47952]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval() #A\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6882e-5242-4f7d-928a-c54497c0a2ec",
   "metadata": {},
   "source": [
    "Using the .decode method of the tokenizer, we can convert the IDs back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69253fd6-c53c-4da6-9525-9408ec40a36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am stadium refuted Relations Nashville weren testifying\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf20ab2-29ed-4820-b4b3-75979a5a173c",
   "metadata": {},
   "source": [
    "As we can see, based on the preceding output, the model generated gibberish, which is not at all coherent text. What happened?\n",
    "\n",
    "The reason why the model is unable to produce coherent text is that we haven't trained it yet. So far, we just implemented the GPT architecture and initialized a GPT model instance with initial random weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f1edf7-579d-48bc-8414-b5bfb9724d99",
   "metadata": {},
   "source": [
    "## 2. Preparing Data for Text Generation\n",
    "\n",
    "We can define two convenience functions, text_to_token_ids and token_ids_to_text, for converting between token and text representations that we can use throughout this chapter. Also, we can use the generate_text_simple function from the previous section to generate text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f33222b2-f753-43aa-a70d-ab2754f8850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f789427-1cb7-45b4-a9ec-2bba7706c29d",
   "metadata": {},
   "source": [
    "We use a relatively small dataset for training the LLM (in fact, only one short story)\n",
    "\n",
    "The reasons are:\n",
    "\n",
    "1. You can run the code examples in a few minutes on a laptop computer without a suitable GPU.\n",
    "\n",
    "2. The training finishes relatively fast (minutes instead of weeks), which is good for educational purposes. We use a text from the public domain, which can be included in this GitHub repository without violating any usage rights or bloating the repository size.\n",
    "\n",
    "3. For example, Llama 2 7B required 184,320 GPU hours on A100 GPUs to be trained on 2 trillion tokens. At the time of this writing, the hourly cost of an 8xA100 cloud server at AWS is approximately 30 dollars. So, via an off-the-envelope calculation, training this LLM would cost 184,320 / 8 * 30 = 690,000 dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "242d6b39-3919-4a23-b0e1-1113c56dacd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 20479\n",
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "    \n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "\n",
    "print(\"Total number of characters:\", total_characters)\n",
    "print(\"Total number of tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6a6c7-d963-4e88-b759-8c26884d9fd6",
   "metadata": {},
   "source": [
    "### Recap: Implementing a Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c18fe38-51ef-42f6-8ee0-c88cc5e502cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f9f619d-f670-41f8-9694-84fede31b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.90 # Train/Validation ratio\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a950d1ca-7243-4937-854a-e6085fb69f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "if total_tokens * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if total_tokens * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d240c545-8cac-4670-8053-3ce33892ab9d",
   "metadata": {},
   "source": [
    "We use a relatively small batch size to reduce the computational resource demand, and because the dataset is very small to begin with. Llama 2 7B was trained with a batch size of 1024, for example.\n",
    "\n",
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50b98df2-3002-4792-a3d9-caf0f883c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c841cff-c6c8-41f7-9c18-89c0d3bb9d0a",
   "metadata": {},
   "source": [
    "An optional check that the data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "760b2929-832e-4e73-ad94-9cb72281d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf09f23-d2c8-4447-9309-7eff0bd9d78e",
   "metadata": {},
   "source": [
    "## 3. Implementing Text Generation Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9a96bf-2207-4e64-bbca-db08214c6348",
   "metadata": {},
   "source": [
    "Suppose we have an inputs tensor containing the token IDs for 2 training examples (rows). Corresponding to the inputs, the targets contain the desired token IDs that we want the model to generate.\n",
    "\n",
    "Notice that the targets are the inputs shifted by 1 position, as explained earlier when we implemented the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f58f2ff-3d92-4b79-96fa-b59efc41bfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86c6b5f-8f62-4144-b52e-d0bc56b1a08f",
   "metadata": {},
   "source": [
    "Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each. Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "\n",
    "Applying the softmax function, we can turn the logits tensor into a tensor of the same dimension containing probability scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ff51a25-98a6-4edd-8a18-2d7ee237caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "print(probas.shape) # Shape: (batch_size, num_tokens, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2cc58-03ea-40b3-a497-914c89d97f50",
   "metadata": {},
   "source": [
    "As discussed earlier, we can apply the argmax function to convert the probability scores into predicted token IDs. The softmax function above produced a 50,257-dimensional vector for each token; the argmax function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token.\n",
    "\n",
    "Since we have 2 input batches with 3 tokens each, we obtain 2 by 3 predicted token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a5fff30-d898-4358-ac8f-a8df62cf9ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[20219],\n",
      "         [44353],\n",
      "         [28138]],\n",
      "\n",
      "        [[48843],\n",
      "         [25885],\n",
      "         [40163]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e6e643-e070-4c63-bc4f-71caa60f3526",
   "metadata": {},
   "source": [
    "If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens. That's because the model wasn't trained yet.\n",
    "\n",
    "To train the model, we need to know how far it is away from the correct predictions (targets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4413658d-ab14-4bb7-b207-536dd41aa7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1: 139consider adjustable\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196f600-04ed-4d6f-b5ed-4056f19206b7",
   "metadata": {},
   "source": [
    "### 1. Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0291a5c-12e9-46b6-8a17-9eaec7150dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([1.0974e-05, 2.2907e-05, 2.3633e-05])\n",
      "Text 2: tensor([7.9591e-06, 2.9679e-05, 1.4502e-05])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7139b6d-8ac7-4061-8e97-50414176b5b7",
   "metadata": {},
   "source": [
    "We want to maximize all these values, bringing them close to a probability of 1. In mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5007ecc1-1966-4c8d-bbc2-768497f1567d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-11.4200, -10.6841, -10.6529, -11.7412, -10.4251, -11.1412])\n"
     ]
    }
   ],
   "source": [
    "# Compute logarithm of all token probabilities\n",
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9003043c-3c3f-4d8f-a69f-7e6adec1799c",
   "metadata": {},
   "source": [
    "Next, we compute the average log probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "260cc8a7-841c-4fd5-8e00-37f0825d00f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-11.0107)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the average probability for each token\n",
    "avg_log_probas = torch.mean(log_probas)\n",
    "print(avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d5c3e-0bec-478c-ba97-1476348be327",
   "metadata": {},
   "source": [
    "The goal is to make this average log probability as large as possible by optimizing the model weights.\n",
    "\n",
    "Due to the log, the largest possible value is 0, and we are currently far away from 0. In deep learning, instead of maximizing the average log-probability, it's a standard convention to minimize the negative average log-probability value; in our case, instead of maximizing -10.7722 so that it approaches 0, in deep learning, we would minimize 10.7722 so that it approaches 0.\n",
    "\n",
    "The value negative of -10.7722, i.e., 10.7722, is also called cross-entropy loss in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4789836f-e499-4932-994f-eadebc6743ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0107)\n"
     ]
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28abdc-91df-4a5f-b942-d9ea3f786ecd",
   "metadata": {},
   "source": [
    "PyTorch already implements a cross_entropy function that carries out the previous steps. Before we apply the cross_entropy function, let's check the shape of the logits and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e324e7d-a60b-400c-a88b-32cede6a2163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([2, 3, 50257])\n",
      "Targets shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Logits have shape (batch_size, num_tokens, vocab_size)\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "# Targets have shape (batch_size, num_tokens)\n",
    "print(\"Targets shape:\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e815897-0471-421f-83cb-0b4280389eff",
   "metadata": {},
   "source": [
    "For the cross_entropy function in PyTorch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61073227-73bc-4397-aa05-062d5effba70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ab0bc-1d1e-47ff-89a5-8fcf0b923d6e",
   "metadata": {},
   "source": [
    "Note that the targets are the token IDs, which also represent the index positions in the logits tensors that we want to maximize. The cross_entropy function in PyTorch will automatically take care of applying the softmax and log-probability computation internally over those token indices in the logits that are to be maximized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b2b14cc3-c019-4e74-9d80-bbbca815a004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.0107)\n"
     ]
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637fdc48-a52d-4dc2-a785-2dc97cf16f1a",
   "metadata": {},
   "source": [
    "### 2. Perplexity\n",
    "\n",
    "A concept related to the cross-entropy loss is the perplexity of an LLM. The perplexity is simply the exponential of the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18e11130-33cd-47bc-b8be-b6368569e3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60521.0195)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19168b-18fb-49c8-9065-715aad072459",
   "metadata": {},
   "source": [
    "The perplexity is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about at each step (in the example above, that'd be 48,725 words or tokens).\n",
    "\n",
    "In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset. Similar to the loss, a lower perplexity indicates that the model predictions are closer to the actual distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d0181e-c774-409c-86d1-07fed7c682bb",
   "metadata": {},
   "source": [
    "### 3. Utility Functions\n",
    "\n",
    "Next, we implement a utility function to calculate the cross-entropy loss of a given batch to be used later. In addition, we implement a second utility function to compute the loss for a user-specified number of batches in a data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "408bedea-f75b-4393-877a-2f2d62e08866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c2743-888e-4077-8520-d242dabf8acf",
   "metadata": {},
   "source": [
    "## 4. Implementing Training Loop for LLM\n",
    "\n",
    "<b>Step 1</b>: Initialize lists to track losses and tokens seen\n",
    "\n",
    "<b>Step 2</b>: Start the main training loop\n",
    "\n",
    "<b>Step 3</b>: Reset loss gradients from previous batch iteration\n",
    "\n",
    "<b>Step 4</b>: Calculate loss gradients\n",
    "\n",
    "<b>Step 5</b>: Update model weights using loss gradients\n",
    "\n",
    "<b>Step 6</b>: Optional evaluation step\n",
    "\n",
    "<b>Step 7</b>: Print a sample text after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da3098ec-7a3e-4b8e-8236-5952c360accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bd89c8-19eb-4db3-acc7-473d10c18dee",
   "metadata": {},
   "source": [
    "The evaluate_model function calculates the loss over the training and validation set while ensuring the model is in evaluation mode with gradient tracking and dropout disabled when calculating the loss over the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "115694b1-b451-495d-8f86-f0d79e27bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cf05aa-dc00-4eb0-9298-257a65de09b3",
   "metadata": {},
   "source": [
    "The generate_and_print_sample function is a convenience function that we use to track whether the model improves during the training.\n",
    "\n",
    "In particular, the generate_and_print_sample function takes a text snippet (start_context) as input, converts it into token IDs, and feeds it to the LLM to generate a text sample using the generate_text_simple function we used earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9258775-4850-473e-a3d4-41c89f5aab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "            \n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward() # Calculate loss gradients\n",
    "            \n",
    "            optimizer.step() # Update model weights using loss gradients\n",
    "            \n",
    "            tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "            global_step += 1\n",
    "\n",
    "            # Evaluation step (Optional)\n",
    "            if global_step % eval_freq == 0: \n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ea2c8d-46f1-43d4-90fd-f3da3f35550a",
   "metadata": {},
   "source": [
    "Let's see this all in action by training a GPTModel instance for 10 epochs using an AdamW optimizer and the train_model_simple function we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9887478-8fa7-4791-afb0-7d59f9e9ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Note:\n",
    "# Uncommenting the following lines will allow the code to run on Apple Silicon chips, if applicable,\n",
    "# which is approximately 2x faster than on an Apple CPU (as measured on an M3 MacBook Air).\n",
    "# However, the resulting loss values may be slightly different.\n",
    "\n",
    "#if torch.cuda.is_available():\n",
    "#    device = torch.device(\"cuda\")\n",
    "#elif torch.backends.mps.is_available():\n",
    "#    device = torch.device(\"mps\")\n",
    "#else:\n",
    "#    device = torch.device(\"cpu\")\n",
    "#\n",
    "# print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d6d2809b-349e-4b78-94fb-5166c60437fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.781, Val loss 9.933\n",
      "Ep 1 (Step 000005): Train loss 8.111, Val loss 8.339\n",
      "Every effort moves you,,,,,,,,,,,,.                                     \n",
      "Ep 2 (Step 000010): Train loss 6.661, Val loss 7.048\n",
      "Ep 2 (Step 000015): Train loss 5.961, Val loss 6.616\n",
      "Every effort moves you, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and, and,, and, and,\n",
      "Ep 3 (Step 000020): Train loss 5.726, Val loss 6.600\n",
      "Ep 3 (Step 000025): Train loss 5.201, Val loss 6.348\n",
      "Every effort moves you, and I had been.                                            \n",
      "Ep 4 (Step 000030): Train loss 4.417, Val loss 6.278\n",
      "Ep 4 (Step 000035): Train loss 4.069, Val loss 6.226\n",
      "Every effort moves you know the                          \"I he had the donkey and I had the and I had the donkey and down the room, I had\n",
      "Ep 5 (Step 000040): Train loss 3.732, Val loss 6.160\n",
      "Every effort moves you know it was not that the picture--I had the fact by the last I had been--his, and in the            \"Oh, and he said, and down the room, and in\n",
      "Ep 6 (Step 000045): Train loss 2.850, Val loss 6.179\n",
      "Ep 6 (Step 000050): Train loss 2.427, Val loss 6.141\n",
      "Every effort moves you know,\" was one of the picture. The--I had a little of a little: \"Yes, and in fact, and in the picture was, and I had been at my elbow and as his pictures, and down the room, I had\n",
      "Ep 7 (Step 000055): Train loss 2.104, Val loss 6.134\n",
      "Ep 7 (Step 000060): Train loss 1.882, Val loss 6.233\n",
      "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I was no--as! The women had been, in the moment--as Jack himself, as once one had been the donkey, and were, and in his\n",
      "Ep 8 (Step 000065): Train loss 1.320, Val loss 6.238\n",
      "Ep 8 (Step 000070): Train loss 0.985, Val loss 6.242\n",
      "Every effort moves you know,\" was one of the axioms he had been the tips of a self-confident moustache, I felt to see a smile behind his close grayish beard--as if he had the donkey. \"strongest,\" as his\n",
      "Ep 9 (Step 000075): Train loss 0.717, Val loss 6.293\n",
      "Ep 9 (Step 000080): Train loss 0.541, Val loss 6.393\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back the window-curtains, I had the donkey. \"There were days when I\n",
      "Ep 10 (Step 000085): Train loss 0.391, Val loss 6.452\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisitely appointed luncheon-table, when, on a later day, I had again run over from Monte Carlo; and Mrs. Gis\n",
      "Training completed in 10.72 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972a5bed-6b94-472b-a1dc-0ee6d5c4ebba",
   "metadata": {},
   "source": [
    "As we can see, based on the results printed during the training, the training loss improves drastically, starting with a value of 9.781 and converging to 0.391. The language skills of the model have improved quite a lot. In the beginning, the model is only able to append commas to the start context (\"Every effort moves you,,,,,,,,,,,,\") or repeat the word \"and\". At the end of the training, it can generate grammatically correct text.\n",
    "\n",
    "Similar to the training set loss, we can see that the validation loss starts high (9.856) and decreases during the training. However, it never becomes as small as the training set loss and remains at 6.372 after the 10th epoch.\n",
    "\n",
    "Let's create a simple plot that shows the training and validation set losses side by side:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b47c7681-eef7-4ec2-8dbc-f76526ba1605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    # plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf8d448-01e7-4658-8751-d66117476d4c",
   "metadata": {},
   "source": [
    "Both the training and validation losses start to improve for the first epoch. However, the losses start to diverge past the second epoch. This divergence and the fact that the validation loss is much larger than the training loss indicate that the model is overfitting to the training data.\n",
    "\n",
    "We can confirm that the model memorizes the training data verbatim by searching for the generated text snippets, such as \"quite insensible to the irony\" in the \"The Verdict\" text file. This memorization is expected since we are working with a very, very small training dataset and training the model for multiple epochs.\n",
    "\n",
    "Usually, it's common to train a model on a much, much larger dataset for only one epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d45593-bbda-45c5-bbe9-c89f75ed1eac",
   "metadata": {},
   "source": [
    "## 5. Loading & Saving Model Weights in PyTorch\n",
    "\n",
    "Fortunately, saving a PyTorch model is relatively straightforward. The recommended way is to save a model's so-called state_dict, a dictionary mapping each layer to its parameters, using the torch.save function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ed40a0b-8ed2-4586-a181-c0289012c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a8e219-745a-4822-90ca-bff644a4697c",
   "metadata": {},
   "source": [
    "In the preceding code, \"model.pth\" is the filename where the state_dict is saved. The .pth extension is a convention for PyTorch files, though we could technically use any file extension.\n",
    "\n",
    "Then, after saving the model weights via the state_dict, we can load the model weights into a new GPTModel model instance as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95ff3714-2c0f-40fb-bf7a-93ed696008c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"./models/model.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7c2e5d-0c52-4835-940a-7ffbb34c2b20",
   "metadata": {},
   "source": [
    "If we plan to continue pretraining a model later, for example, using the train_model_simple function we defined earlier in this chapter, saving the optimizer state is also recommended.\n",
    "\n",
    "Adaptive optimizers such as AdamW store additional parameters for each model weight. AdamW uses historical data to adjust learning rates for each model parameter dynamically. Without it, the optimizer resets, and the model may learn suboptimally or even fail to converge properly, which means that it will lose the ability to generate coherent text.\n",
    "\n",
    "Using torch.save, we can save both the model and optimizer state_dict contents as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c320996-3e70-46e4-82c7-a71e4aa1be9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \n",
    "    \"./models/model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96353dec-5f24-4b8e-bf36-446d3aa3ab63",
   "metadata": {},
   "source": [
    "Then, we can restore the model and optimizer states as follows by first loading the saved data via torch.load and then using the load_state_dict method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d17ce755-6509-4276-84b0-4cc6933b6da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./models/model_and_optimizer.pth\")\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce9a061-92d8-48fa-b605-02e9f7c109f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
