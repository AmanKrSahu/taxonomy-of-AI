{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2c16cf-45c8-4445-8331-bb2dd1cdbd84",
   "metadata": {},
   "source": [
    "# Data Preparation in LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e60347-ae88-4b83-b108-9620691302bd",
   "metadata": {},
   "source": [
    "## 1. Overview of Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c55902-dd58-40fa-879a-3ff5429cb18b",
   "metadata": {},
   "source": [
    "### Step 1: Extract data from the source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6faf7a-d713-45c9-9e4f-8a5522f3d430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character in file: 20479\n"
     ]
    }
   ],
   "source": [
    "with open(\"../datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character in file:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2209f-8917-4558-bdc2-3bfad89dab43",
   "metadata": {},
   "source": [
    "Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into embeddings for LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9727652b-daf1-4c98-ad6c-6ca15af6b713",
   "metadata": {},
   "source": [
    "### Step 2: Convert the text into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6675e24f-015f-4e86-82b4-bf3c62d43dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world. This, is a test.\"\n",
    "\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c1883-65d6-45a8-8dee-f192263c7ad8",
   "metadata": {},
   "source": [
    "Now that we got a basic tokenizer working, let's apply it to Edith Wharton's entire short story."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d3bdad8-5c37-4925-8189-0c00f8179ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 4690\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "\n",
    "print(\"Total number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583daa5-0e7c-4614-a1a3-a334c7f97414",
   "metadata": {},
   "source": [
    "### Step 3: Convert tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f410d2e7-1ca8-4d0d-8962-3745df2543f6",
   "metadata": {},
   "source": [
    "In the previous section, we tokenized Edith Wharton's short story and assigned it to a Python variable called preprocessed. Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc1325b-64af-44b7-b10e-672a4a0dc8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 1130\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Total vocabulary size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2326c7-0c6e-47ab-a6cb-6d4e1cd304f9",
   "metadata": {},
   "source": [
    "After determining that the vocabulary size is 1,130 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fdc7233-0d58-4ee0-bff0-f635944df15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n",
      "('--', 6)\n",
      "('.', 7)\n",
      "(':', 8)\n",
      "(';', 9)\n",
      "('?', 10)\n",
      "('A', 11)\n",
      "('Ah', 12)\n",
      "('Among', 13)\n",
      "('And', 14)\n",
      "('Are', 15)\n",
      "('Arrt', 16)\n",
      "('As', 17)\n",
      "('At', 18)\n",
      "('Be', 19)\n",
      "('Begin', 20)\n",
      "('Burlington', 21)\n",
      "('But', 22)\n",
      "('By', 23)\n",
      "('Carlo', 24)\n",
      "('Chicago', 25)\n",
      "('Claude', 26)\n",
      "('Come', 27)\n",
      "('Croft', 28)\n",
      "('Destroyed', 29)\n",
      "('Devonshire', 30)\n",
      "('Don', 31)\n",
      "('Dubarry', 32)\n",
      "('Emperors', 33)\n",
      "('Florence', 34)\n",
      "('For', 35)\n",
      "('Gallery', 36)\n",
      "('Gideon', 37)\n",
      "('Gisburn', 38)\n",
      "('Gisburns', 39)\n",
      "('Grafton', 40)\n",
      "('Greek', 41)\n",
      "('Grindle', 42)\n",
      "('Grindles', 43)\n",
      "('HAD', 44)\n",
      "('Had', 45)\n",
      "('Hang', 46)\n",
      "('Has', 47)\n",
      "('He', 48)\n",
      "('Her', 49)\n",
      "('Hermia', 50)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
    "\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb42482-0468-431a-860c-3778d6e4eb40",
   "metadata": {},
   "source": [
    "As we can see, based on the output above, the dictionary contains individual tokens associated with unique integer labels. \n",
    "\n",
    "When we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa2aca-3091-46c0-b78d-5f0d73777672",
   "metadata": {},
   "source": [
    "### Demo: Tokenizer Class in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4756e80-8e47-425b-b8da-06459488fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "                                \n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b255135-1727-44ad-80d2-462e35325b3f",
   "metadata": {},
   "source": [
    "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 class and tokenize a passage from Edith Wharton's short story to try it out in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f7bcb1-a0b4-4386-8ffc-53eba804ab42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89595e6-1555-42c4-9fcf-a791af910020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee87314-e141-40bb-a1ed-9a76f5f681c1",
   "metadata": {},
   "source": [
    "So far, so good. We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set.\n",
    "\n",
    "Let's now apply it to a new text sample that is not contained in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ae654e-3a73-42e3-96ff-c09c548e380d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Hello'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m text = \u001b[33m\"\u001b[39m\u001b[33mHello, do you like tea?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mSimpleTokenizerV1.encode\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m      7\u001b[39m preprocessed = re.split(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m([,.:;?_!\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m()\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[33m]|--|\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms)\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m      9\u001b[39m preprocessed = [\n\u001b[32m     10\u001b[39m     item.strip() \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m preprocessed \u001b[38;5;28;01mif\u001b[39;00m item.strip()\n\u001b[32m     11\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m ids = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstr_to_int\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m preprocessed]\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "\u001b[31mKeyError\u001b[39m: 'Hello'"
     ]
    }
   ],
   "source": [
    "text = \"Hello, do you like tea?\"\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c148c1-a793-4c01-8acf-22b27eaf6aa3",
   "metadata": {},
   "source": [
    "<b><u>IMPORANT</u></b>: The problem is that the word \"Hello\" was not used in the The Verdict short story. Hence, it is not contained in the vocabulary. This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cc152-cac5-4315-978d-de071ac1b7c1",
   "metadata": {},
   "source": [
    "### Demo: Adding Special Context Tokens\n",
    "\n",
    "In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set. In this section, we will modify this tokenizer to handle unknown words.\n",
    "\n",
    "In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2, to support two new tokens, <|unk|> and <|endoftext|>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7291d74-f319-453f-8f48-307304e814ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 1132\n"
     ]
    }
   ],
   "source": [
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
    "\n",
    "print(\"Total vocabulary size:\", len(vocab.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a252dc1-a896-4762-a6b8-e6b99e1f6af0",
   "metadata": {},
   "source": [
    "Based on the output of the print statement above, the new vocabulary size is 1132 (the vocabulary size in the previous section was 1130)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f1fb5e5-e05d-4351-8899-d123dbe73f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "182497c3-7f11-4930-8c37-c8e5bab22aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"In the sunlit terraces of the palace.\"\n",
    "\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82c339a6-8b9c-4559-a8b6-42798008914d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0452bb-3dd7-4eb2-997e-ad56892fc970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304debbb-a399-49b2-8b18-bb0182c151b9",
   "metadata": {},
   "source": [
    "So far, we have discussed tokenization as an essential step in processing text as input to LLMs. Depending on the LLM, some researchers also consider additional special tokens such as the following:\n",
    "\n",
    "[BOS] (beginning of sequence): This token marks the start of a text. It signifies to the LLM where a piece of content begins.\n",
    "\n",
    "[EOS] (end of sequence): This token is positioned at the end of a text, and is especially useful when concatenating multiple unrelated texts, similar to <|endoftext|>. For instance, when combining two different Wikipedia articles or books, the [EOS] token indicates where one article ends and the next one begins.\n",
    "\n",
    "[PAD] (padding): When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or \"padded\" using the [PAD] token, up to the length of the longest text in the batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764377a0-e93c-4a3a-9a75-346be5454b5d",
   "metadata": {},
   "source": [
    "## 2. Byte-Pair Encoding (Sub-Word Tokenization)\n",
    "\n",
    "We implemented a simple tokenization scheme in the previous sections for illustration purposes. This section covers a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE).\n",
    "\n",
    "The BPE tokenizer covered in this section was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.\n",
    "\n",
    "Since implementing BPE can be relatively complicated, we will use an existing Python open-source library called tiktoken (https://github.com/openai/tiktoken).\n",
    "\n",
    "This library implements the BPE algorithm very efficiently based on source code in Rust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b667cd98-eca8-47b2-9b1c-2f792ed8f2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fd137-9aa6-48ff-908d-35a7a92781b9",
   "metadata": {},
   "source": [
    "Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8dfdb80e-b342-49a7-804c-54eacff78668",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c984e9-4372-4d2e-bc1b-9cba9a601d2c",
   "metadata": {},
   "source": [
    "The usage of this tokenizer is similar to SimpleTokenizerV2 we implemented previously via an encode method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "688ef6fb-eee8-4e37-b649-5bb50a5b4572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6bc49a-44d9-4f37-bb47-8f0c557f131c",
   "metadata": {},
   "source": [
    "We can then convert the token IDs back into text using the decode method, similar to our SimpleTokenizerV2 earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e1134c8-e7bf-4215-beea-96195d9d830c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95dcd9d-1b3e-45b5-bd29-d40f6fbecc18",
   "metadata": {},
   "source": [
    "The algorithm underlying BPE breaks down words that aren't in its predefined vocabulary into smaller subword units or even individual characters.\n",
    "\n",
    "The enables it to handle out-of-vocabulary words.\n",
    "\n",
    "So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb640b95-1931-4daf-8552-2a74a8c1f114",
   "metadata": {},
   "source": [
    "### Demo: Unknown Tokens using BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fec26500-b48f-48d9-b599-25250c49912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64f577ce-31ba-4c0e-8a2e-541f2009dc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50257\n",
      "The vocabulary size for GPT3 is: 50281\n",
      "The vocabulary size for GPT4 is: 100277\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"),  # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\")  # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d4d908-32a4-44d1-bb55-4e134a1f27cc",
   "metadata": {},
   "source": [
    "## 3. Creating Input-Target Pairs\n",
    "\n",
    "In this section we implement a data loader that fetches the input-target pairs using a sliding window approach.\n",
    "\n",
    "To get started, we will first tokenize the whole The Verdict short story we worked with earlier using the BPE tokenizer introduced in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92b10034-883b-4af5-b93d-fa48ae09682c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in file: 5145\n"
     ]
    }
   ],
   "source": [
    "with open(\"../datasets/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(\"Total number of tokens in file:\", len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d033fcc0-14b0-461d-af77-7a5adb9a5e3b",
   "metadata": {},
   "source": [
    "One of the easiest and most intuitive ways to create the input-target pairs for the nextword prediction task is to create two variables, x and y, where x contains the input tokens and y contains the targets, which are the inputs shifted by 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14ac0e-8005-40c2-acaf-18863f124b7c",
   "metadata": {},
   "source": [
    "<b><u>NOTE</u></b>: The context size determines how many tokens are included in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f2e038c-419a-4c17-a302-46e27aeca816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:      [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4 #length of the input\n",
    "\n",
    "\"\"\"\n",
    "The context_size of 4 means that the model is trained to look at a sequence of 4 words (or tokens) to predict the next word in the sequence. \n",
    "\n",
    "The input x is the first 4 tokens [1, 2, 3, 4], and the target y is the next 4 tokens [2, 3, 4, 5]\n",
    "\"\"\"\n",
    "\n",
    "x = enc_text[:context_size]\n",
    "y = enc_text[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388eeda3-fefb-47f0-93aa-eb22dc31fa75",
   "metadata": {},
   "source": [
    "Processing the inputs along with the targets, which are the inputs shifted by one position, we can then create the next-word prediction tasks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa629efd-e46f-4bc4-bd21-fe0ddea0942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] ----> 367\n",
      "[40, 367] ----> 2885\n",
      "[40, 367, 2885] ----> 1464\n",
      "[40, 367, 2885, 1464] ----> 1807\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "\n",
    "    print(context, \"---->\", desired)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b95e0-d90e-4bd4-bf7d-85851823e6c8",
   "metadata": {},
   "source": [
    "Everything left of the arrow (---->) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict.\n",
    "\n",
    "For illustration purposes, let's repeat the previous code but convert the token IDs into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51e38eb6-bdd0-49fd-961e-eb1dbbc54266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ---->  H\n",
      "I H ----> AD\n",
      "I HAD ---->  always\n",
      "I HAD always ---->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, context_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4474cadb-cb2f-461b-b775-5c8ac7c46058",
   "metadata": {},
   "source": [
    "In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3618323b-2e60-4669-8030-9bf12c08db66",
   "metadata": {},
   "source": [
    "## 4. Implementing a Data Loader\n",
    "\n",
    "For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a8f5b-31ce-4eef-a23e-6f2f5e7c425e",
   "metadata": {},
   "source": [
    "### Demo: PyTorch Dataset\n",
    "\n",
    "<b>Step 1</b>: Tokenize the entire text.\n",
    "\n",
    "<b>Step 2</b>: Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "\n",
    "<b>Step 3</b>: Return the total number of rows in the dataset.\n",
    "\n",
    "<b>Step 4</b>: Return a single row from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd0190a4-97e4-45dd-a8b3-1d792bfa1beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class DatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c85631-3c4c-49ea-94a8-831ba59276bf",
   "metadata": {},
   "source": [
    "The DatasetV1 class in listing is based on the PyTorch Dataset class. It defines how individual rows are fetched from the dataset.\n",
    "\n",
    "Each row consists of a number of token IDs (based on a max_length) assigned to an input_chunk tensor. The target_chunk tensor contains the corresponding targets.\n",
    "\n",
    "I recommend reading on to see how the data returned from this dataset looks like when we combine the dataset with a PyTorch DataLoader, this will bring additional intuition and clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f0bec-1709-4f83-968b-1b49f247125b",
   "metadata": {},
   "source": [
    "### Demo: PyTorch DataLoader\n",
    "\n",
    "The following code will use the DatasetV1 to load the inputs in batches via a PyTorch DataLoader:\n",
    "\n",
    "<b>Step 1</b>: Initialize the tokenizer\n",
    "\n",
    "<b>Step 2</b>: Create dataset\n",
    "\n",
    "<b>Step 3</b>: drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "\n",
    "<b>Step 4</b>: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca89cb29-fe00-48bc-a915-1b4add9d7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = DatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acae315-4840-46d0-8782-5e11f47ce145",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4,\n",
    "\n",
    "This will develop an intuition of how the DatasetV1 class and the create_dataloader_v1 function work together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ec19ab8a-f9d8-4fce-ac3d-49c52e62e22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cpu\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fd27c-40e7-49f9-90f8-0b8c73c4494d",
   "metadata": {},
   "source": [
    "The first_batch variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs.\n",
    "\n",
    "Since the max_length is set to 4, each of the two tensors contains 4 token IDs.\n",
    "\n",
    "Note that an input size of 4 is relatively small and only chosen for illustration purposes. It is common to train LLMs with input sizes of at least 256."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57fddb6-673f-464a-a7d6-0526564cc582",
   "metadata": {},
   "source": [
    "Before we move on to the two final sections of this chapter that are focused on creating the embedding vectors from the token IDs, let's have a brief look at how we can use the data loader to sample with a batch size greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7e39c87-7f44-4d25-9d20-648fbd0273cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f4fb16-a84c-4213-a82c-09e796ca9aa3",
   "metadata": {},
   "source": [
    "<b><u>NOTE</u></b>: We increase the stride to 4. This is to utilize the data set fully (we don't skip a single word) but also avoid any overlap between the batches, since more overlap could lead to increased overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac395d-ff94-4a4f-ab8e-1efb9a79719c",
   "metadata": {},
   "source": [
    "## 5. Creating Token Embeddings\n",
    "\n",
    "Let's illustrate how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d69715d9-8d36-41d4-ba59-e201613b4d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([2, 3, 5, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d700ab6-3e0d-4c6f-bb0f-f67a563b91d3",
   "metadata": {},
   "source": [
    "For the sake of simplicity and illustration purposes, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions).\n",
    "\n",
    "Using the vocab_size and output_dim, we can instantiate an embedding layer in PyTorch, setting the random seed to 123 for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b79ccb0-7eb9-438c-9337-1de2d5c27796",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5c0419-057b-4606-af83-5c91c6476c76",
   "metadata": {},
   "source": [
    "The print statement in the code prints the embedding layer's underlying weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d119d01-8e32-4c94-b698-62c61968fd8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f4a69-d2bd-476f-838b-7981d67dbf33",
   "metadata": {},
   "source": [
    "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions.\n",
    "\n",
    "After we instantiated the embedding layer, let's now apply it to a token ID to obtain the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9ceaaa75-55da-46b6-9c42-eb01eb8a9754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62701795-b1da-47e4-a51d-dee31d48644a",
   "metadata": {},
   "source": [
    "If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the 4th row (Python starts with a zero index, so it's the row corresponding to index 3). In other words, the embedding layer is essentially a look-up operation that retrieves rows from the embedding layer's weight matrix via a token ID.\n",
    "\n",
    "Previously, we have seen how to convert a single token ID into a three-dimensional embedding vector. Let's now apply that to all four input IDs we defined earlier (torch.tensor([2, 3, 5, 1]))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75abd491-57b1-49e5-878c-297c21e06c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f180f57-c2bd-411f-b87a-2d8e57359868",
   "metadata": {},
   "source": [
    "Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770c5973-e5ab-40f5-9232-7ec47522e799",
   "metadata": {},
   "source": [
    "## 6. Creating Positional Embeddings\n",
    "\n",
    "Previously, we focused on very small embedding sizes in this chapter for illustration purposes.\n",
    "\n",
    "We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation.\n",
    "\n",
    "This is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation.\n",
    "\n",
    "Furthermore, we assume that the token IDs were created by the BPE tokenizer that we implemented earlier, which has a vocabulary size of 50,257."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c09e59b7-f601-4e9f-9266-632afef99afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9744c8b5-f333-4319-a3cd-f9baf13979e9",
   "metadata": {},
   "source": [
    "Using the token_embedding_layer above, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 x 4 x 256 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23221bc4-0cfd-43e3-b3d9-130821e4aa95",
   "metadata": {},
   "source": [
    "Let's instantiate the data loader ( Data sampling with a sliding window), first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b712e4d-1dd6-499b-a3be-e847829e2f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_length = 4\n",
    "\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a22e78-abe7-43f8-a8d6-00027ab13066",
   "metadata": {},
   "source": [
    "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data batch consists of 8 text samples with 4 tokens each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b8850-62f8-4e77-8ed3-fdb3d431afa8",
   "metadata": {},
   "source": [
    "Let's now use the embedding layer to embed these token IDs into 256-dimensional vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c173b68e-6677-4913-bc05-11834f11a614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdd711-6620-40ff-8989-a320d5171576",
   "metadata": {},
   "source": [
    "As we can tell based on the 8x4x256-dimensional tensor output, each token ID is now embedded as a 256-dimensional vector.\n",
    "\n",
    "For a GPT model's absolute embedding approach, we just need to create another embedding layer that has the same dimension as the token_embedding_layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a05e10f6-12f3-4d41-9134-92751848e4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7c9b4-8349-47e4-b5af-08f49a6c1c73",
   "metadata": {},
   "source": [
    "As shown in the preceding code example, the input to the pos_embeddings is usually a placeholder vector torch.arange(context_length), which contains a sequence of numbers 0, 1, ..., up to the maximum input length âˆ’ 1. The context_length is a variable that represents the supported input size of the LLM. Here, we choose it similar to the maximum length of the input text.\n",
    "\n",
    "In practice, input text can be longer than the supported context length, in which case we have to truncate the text.\n",
    "\n",
    "As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4x256- dimensional pos_embeddings tensor to each 4x256-dimensional token embedding tensor in each of the 8 batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "639299aa-0926-412d-bc39-2fba3b3897e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2fa80-23a4-4bb9-bcff-38dd82f226df",
   "metadata": {},
   "source": [
    "The input_embeddings we created are the embedded input examples that can now be processed by the main LLM modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40574ac5-ddf3-4a7b-9d54-7593d0e6db09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
